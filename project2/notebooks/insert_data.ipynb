{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d82ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "from datetime import datetime\n",
    "from dotenv import dotenv_values\n",
    "from sqlalchemy import create_engine\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = dotenv_values(\"../../.env.local\")['INFLUXDB_TOKEN']\n",
    "org = \"my-org\"\n",
    "bucket = \"meets\"\n",
    "url = \"http://localhost:8086\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48014b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conex√£o\n",
    "client = InfluxDBClient(url=url, token=token, org=org)\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9488a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_or_none(value):\n",
    "    try:\n",
    "        if value is None:\n",
    "            return None\n",
    "        return int(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "    \n",
    "def str_or_none(value):\n",
    "    try:\n",
    "        if value is None:\n",
    "            return None\n",
    "        return str(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def float_or_none(value):\n",
    "    try:\n",
    "        if value is None:\n",
    "            return None\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meets\n",
    "df_meets = pd.read_parquet(\"../../data/meetings/meetings.parquet\")\n",
    "df_meets[\"meeting_key\"] = df_meets[\"meeting_key\"].astype(str)\n",
    "df_meets[\"date_start\"] = pd.to_datetime(df_meets[\"date_start\"])\n",
    "\n",
    "for _, row in tqdm(df_meets.iterrows(), total=len(df_meets)):\n",
    "    p = (\n",
    "        Point(\"meetings\")\n",
    "        .tag(\"meeting_key\", str_or_none(row[\"meeting_key\"]))\n",
    "        .tag(\"country_code\", str_or_none(row[\"country_code\"]))\n",
    "        .tag(\"circuit_short_name\", str_or_none(row[\"circuit_short_name\"]))\n",
    "        .tag(\"meeting_name\", str_or_none(row[\"meeting_name\"]))\n",
    "        .tag(\"meeting_official_name\", str_or_none(row[\"meeting_official_name\"]))\n",
    "        .tag(\"location\", str_or_none(row[\"location\"]))\n",
    "        .tag(\"country_key\", int_or_none(row[\"country_key\"]))\n",
    "        .tag(\"country_name\", str_or_none(row[\"country_name\"]))\n",
    "        .field(\"circuit_key\", int_or_none(row[\"circuit_key\"]))\n",
    "        .field(\"gmt_offset\", str_or_none(row[\"gmt_offset\"]))\n",
    "        .field(\"year\", int_or_none(row[\"year\"]))\n",
    "        .time(row[\"date_start\"], WritePrecision.NS)\n",
    "    )\n",
    "    write_api.write(bucket=bucket, org=org, record=p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4257fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather Conditions\n",
    "df_weather = pd.read_parquet(\"../../data/weather_conditions/weather_conditions.parquet\")\n",
    "df_session = pd.read_parquet(\"../../data/sessions/sessions.parquet\")\n",
    "df_weather = df_weather.drop(columns=[\"meeting_key\"])\n",
    "df_weather = df_weather.drop_duplicates(subset=[\"session_key\", \"date\"])\n",
    "df_weather = df_weather[df_weather[\"session_key\"].isin(df_session[\"session_key\"].to_list())]\n",
    "df_weather[\"rainfall\"] = df_weather[\"rainfall\"].astype(bool)\n",
    "\n",
    "for _, row in tqdm(df_weather.iterrows(), total=len(df_weather)):\n",
    "    point = (\n",
    "        Point(\"weather_conditions\")\n",
    "        .tag(\"session_key\", str_or_none(row[\"session_key\"]))\n",
    "        .field(\"track_temperature\", (row[\"track_temperature\"]))\n",
    "        .field(\"wind_speed\", row[\"wind_speed\"])\n",
    "        .field(\"rainfall\", int(row[\"rainfall\"]))  \n",
    "        .field(\"humidity\", row[\"humidity\"])\n",
    "        .field(\"pressure\", row[\"pressure\"])\n",
    "        .field(\"air_temperature\", row[\"air_temperature\"])\n",
    "        .field(\"wind_direction\", row[\"wind_direction\"]) \n",
    "        .time(row[\"date\"], WritePrecision.NS)\n",
    "    )\n",
    "    write_api.write(bucket=bucket, org=org, record=point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41650826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sessions\n",
    "df_sessions = pd.read_parquet(\"../../data/sessions/sessions.parquet\")\n",
    "\n",
    "df_sessions[\"date_start\"] = pd.to_datetime(df_sessions[\"date_start\"])\n",
    "df_sessions[\"date_end\"] = pd.to_datetime(df_sessions[\"date_end\"])\n",
    "\n",
    "for _, row in tqdm(df_sessions.iterrows(), total=len(df_sessions)):\n",
    "    point = (\n",
    "        Point(\"sessions\")\n",
    "        .tag(\"session_key\", str_or_none(row[\"session_key\"]))\n",
    "        .tag(\"meeting_key\", str_or_none(row[\"meeting_key\"]))\n",
    "        .tag(\"location\", str_or_none(row[\"location\"]))\n",
    "        .tag(\"circuit_short_name\", str_or_none(row[\"circuit_short_name\"]))\n",
    "        .tag(\"session_type\", str_or_none(row[\"session_type\"]))  \n",
    "        .tag(\"session_name\", str_or_none(row[\"session_name\"]))\n",
    "        .field(\"year\", row[\"year\"])\n",
    "        .time(row[\"date_start\"], WritePrecision.NS)\n",
    "    )\n",
    "    write_api.write(bucket=bucket, org=org, record=point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987eb575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drivers\n",
    "df_drivers = pd.read_parquet(\"../../data/drivers/drivers.parquet\")\n",
    "df_drivers = df_drivers.drop(columns=[\"meeting_key\"])\n",
    "\n",
    "for _, row in tqdm(df_drivers.iterrows(), total=len(df_drivers)):\n",
    "    point = (\n",
    "        Point(\"drivers\")\n",
    "        .tag(\"session_key\", str_or_none(row[\"session_key\"]))\n",
    "        .field(\"driver_number\", str_or_none(row[\"driver_number\"]))\n",
    "        .tag(\"broadcast_name\", str_or_none(row[\"broadcast_name\"]))\n",
    "        .tag(\"full_name\", str_or_none(row[\"full_name\"]))\n",
    "        .tag(\"name_acronym\", str_or_none(row[\"name_acronym\"]))\n",
    "        .tag(\"team_name\", str_or_none(row[\"team_name\"]))\n",
    "        .tag(\"team_colour\", str_or_none(row[\"team_colour\"]))\n",
    "        .tag(\"first_name\", str_or_none(row[\"first_name\"]))\n",
    "        .tag(\"last_name\", str_or_none(row[\"last_name\"]))\n",
    "        .tag(\"headshot_url\", str_or_none(row[\"headshot_url\"]))\n",
    "        .tag(\"country_code\", str_or_none(row[\"country_code\"]))\n",
    "        .time(pd.Timestamp.utcnow(), WritePrecision.NS)\n",
    "    )\n",
    "    write_api.write(bucket=bucket, org=org, record=point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tyre_stints\n",
    "df_tyre_strints = pd.read_parquet(\"../../data/stints/stints.parquet\")\n",
    "df_tyre_strints = df_tyre_strints.drop(columns=[\"meeting_key\"])\n",
    "df_tyre_strints = df_tyre_strints.drop_duplicates(subset=[\"session_key\", \"stint_number\", \"driver_number\"])\n",
    "\n",
    "df_tyre_strints[\"lap_start\"] = pd.to_datetime(df_tyre_strints[\"lap_start\"])\n",
    "\n",
    "for _, row in tqdm(df_tyre_strints.iterrows(), total=len(df_tyre_strints)):\n",
    "    try:\n",
    "        point = (\n",
    "            Point(\"tyre_stints\")\n",
    "            .tag(\"driver_number\", str_or_none(row[\"driver_number\"]))\n",
    "            .tag(\"session_key\", str_or_none(row[\"session_key\"]))\n",
    "            .tag(\"compound\", str_or_none(row[\"compound\"]))\n",
    "            .field(\"stint_number\", int(row[\"stint_number\"]))\n",
    "            .field(\"tyre_age_at_start\", float(row[\"tyre_age_at_start\"]) if not pd.isna(row[\"tyre_age_at_start\"]) else None)\n",
    "            .time(df_tyre_strints[\"lap_start\"], WritePrecision.NS)\n",
    "        )\n",
    "        write_api.write(bucket=bucket, org=org, record=point)\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f4dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_laps = pd.read_parquet(\"../../data/laps/laps.parquet\")\n",
    "df_laps = df_laps.drop(columns=[\"meeting_key\"])\n",
    "df_laps = df_laps.drop(columns=[\"segments_sector_1\", \"segments_sector_2\", \"segments_sector_3\"])\n",
    "\n",
    "df_laps[\"date_start\"] = pd.to_datetime(df_laps[\"date_start\"], format='ISO8601', utc=True)\n",
    "\n",
    "for _, row in tqdm(df_laps.iterrows(), total=len(df_laps)):\n",
    "    point = (\n",
    "        Point(\"laps\")\n",
    "        .tag(\"session_key\", str_or_none(row[\"session_key\"]))\n",
    "        .tag(\"driver_number\", str_or_none(row[\"driver_number\"]))\n",
    "        .tag(\"is_pit_out_lap\", str_or_none(row[\"is_pit_out_lap\"]))\n",
    "        .field(\"i1_speed\", float_or_none(row[\"i1_speed\"]))\n",
    "        .field(\"i2_speed\", float_or_none(row[\"i2_speed\"]))\n",
    "        .field(\"st_speed\", float_or_none(row[\"st_speed\"]))\n",
    "        .field(\"lap_duration\", float_or_none(row[\"lap_duration\"]))\n",
    "        .field(\"duration_sector_1\", float_or_none(row[\"duration_sector_1\"]))\n",
    "        .field(\"duration_sector_2\", float_or_none(row[\"duration_sector_2\"]))\n",
    "        .field(\"duration_sector_3\", float_or_none(row[\"duration_sector_3\"]))\n",
    "        .field(\"lap_number\", int_or_none(row[\"lap_number\"]))\n",
    "        .time(row[\"date_start\"], WritePrecision.NS)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pits = pd.read_parquet(\"../../data/pits/pits.parquet\")\n",
    "df_pits = df_pits.drop(columns=[\"meeting_key\"])\n",
    "\n",
    "df_pits[\"date\"] = pd.to_datetime(df_pits[\"date\"], format='ISO8601', utc=True)\n",
    "\n",
    "for _, row in tqdm(df_pits.iterrows(), total=len(df_pits)):\n",
    "    point = (\n",
    "        Point(\"pits\")\n",
    "        .tag(\"session_key\", str_or_none(row[\"session_key\"]))\n",
    "        .tag(\"driver_number\", str_or_none(row[\"driver_number\"]))\n",
    "        .tag(\"lap_number\", str_or_none(row[\"lap_number\"]))\n",
    "        .field(\"pit_duration\", float_or_none(row[\"pit_duration\"]))\n",
    "        .time(row[\"date\"], WritePrecision.NS)\n",
    "    )\n",
    "    write_api.write(bucket=bucket, org=org, record=point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb7eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positions = pd.read_parquet(\"../../data/positions/positions.parquet\")\n",
    "df_positions = df_positions.drop(columns=[\"meeting_key\"])\n",
    "df_positions = df_positions.drop_duplicates(subset=[\"session_key\", \"driver_number\", \"date\"])\n",
    "\n",
    "df_positions[\"date\"] = pd.to_datetime(df_positions[\"date\"], format='ISO8601', utc=True)\n",
    "\n",
    "for _, row in tqdm(df_positions.iterrows(), total=len(df_positions)):\n",
    "    point = (\n",
    "        Point(\"positions\")\n",
    "        .tag(\"session_key\", str_or_none(row[\"session_key\"]))\n",
    "        .tag(\"driver_number\", str_or_none(row[\"driver_number\"]))\n",
    "        .field(\"position\", int(row[\"position\"]))\n",
    "        .time(row[\"date\"], WritePrecision.NS)\n",
    "    )\n",
    "    write_api.write(bucket=bucket, org=org, record=point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c719183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_telemetry(file_path):\n",
    "    try:\n",
    "        client = InfluxDBClient(url=url, token=token, org=org)\n",
    "        write_api = client.write_api(write_options=SYNCHRONOUS)\n",
    "\n",
    "        df_telemetry = pd.read_parquet(file_path)\n",
    "        print(f\"Processing file: {file_path} {df_telemetry.shape}\")\n",
    "\n",
    "        if \"meeting_key\" in df_telemetry.columns:\n",
    "            df_telemetry = df_telemetry.drop(columns=[\"meeting_key\"])\n",
    "        \n",
    "        df_telemetry = df_telemetry.drop_duplicates(subset=[\"session_key\", \"driver_number\", \"date\"])\n",
    "        \n",
    "        df_telemetry[\"date\"] = pd.to_datetime(df_telemetry[\"date\"], format='ISO8601', utc=True)\n",
    "\n",
    "        for _, row in df_telemetry.iterrows():\n",
    "            point = (\n",
    "                Point(\"telemetry\")\n",
    "                .tag(\"driver_number\", str_or_none(row[\"driver_number\"]))\n",
    "                .tag(\"session_key\", str_or_none(row[\"session_key\"]))\n",
    "                .field(\"rpm\", int(row[\"rpm\"]))\n",
    "                .field(\"speed\", int(row[\"speed\"]))\n",
    "                .field(\"n_gear\", int(row[\"n_gear\"]))\n",
    "                .field(\"throttle\", int(row[\"throttle\"]))\n",
    "                .field(\"brake\", int(row[\"brake\"]))\n",
    "                .field(\"drs\", int(row[\"drs\"]))\n",
    "                .time(row[\"date\"], WritePrecision.NS)\n",
    "            )\n",
    "            write_api.write(bucket=bucket, org=org, record=point)\n",
    "    except:\n",
    "        print(f\"Error processing file {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_telemetrys = \"../../data/telemetrys\"\n",
    "if not os.path.isdir(path_telemetrys):\n",
    "    raise FileNotFoundError(f\"Directory does not exist: {path_telemetrys}\")\n",
    "\n",
    "files = [f for f in os.listdir(path_telemetrys) if f.endswith(\".parquet\")]\n",
    "\n",
    "Parallel(n_jobs=-1)(\n",
    "    delayed(process_telemetry)(os.path.join(path_telemetrys, file))\n",
    "    for file in files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda55a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
